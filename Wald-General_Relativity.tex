\documentclass{note}

\usepackage{mathrsfs}
\usepackage{geometry}
\usepackage{xcolor}
\usepackage{fontspec}
\usepackage{titlesec}
\usepackage{fmtcount}


\titleformat{\chapter}[display]
    {\fontspec{Times New Roman}}
    {\NUMBERstring{chapter}}
    {1pt}
    {\hrule\vspace{6pt}\Large\fontspec{Times New Roman}\MakeUppercase}
\renewcommand{\thesection}{\thechapter.\arabic{section}}
\titleformat{\section}[block]
    {\Large\bfseries}
    {\thesection}
    {1em}
    {\fontspec{Times New Roman}\selectfont}


\numberwithin{equation}{chapter}
\makeatletter
\def\tagform@#1{\maketag@@@{[\ignorespaces#1\unskip\@@italiccorr]}}
\makeatother

\newcounter{exercise}[chapter]
\newcommand\Ex{
    \if@noskipsec \leavevmode \fi
    \par
    \refstepcounter{exercise}
    \noindent{\bfseries\arabic{exercise}.}
}

\renewcommand{\theenumi}{\alph{enumi}}

\newcommand{\vat}[2]{\left.#1\right\rvert_{#2}}
\newcommand{\pdv}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\pdvf}[2]{\frac{\partial}{\partial #1}\prn{#2}}


\begin{document}

\chapter{Introduction}

\section*{Problem}

\Page{9}

\Ex \textit{Car and garage paradox.} We set up global inertial coordinates $t,
    x, y, z$ for the doorman and $t', x', y', z'$ for the driver. Assume that the
car travels in the $x$ and $x'$ directions in the two systems, that the garage
door is located at $x = y = z = 0$ in the doorman's coordinates and the back
end of the car is located at $x' = y' = z' = 0$ in the driver's coordinates,
and that the back end of the car enters the garage at time $t = t' = 0$. Also
assume that the car and the garage have the same length L.

We note down the Lorentz transform:
\begin{align}
    t' & = (t - vx/c^2)/(1-v^2/c^2)^{1/2}, \label{Lorentz-1} \\
    x' & = (x - vt)/(1 - v^2/c^2)^{1/2}. \label{Lorentz-2}
\end{align}

At $t = 0$, we assume the doorman sees the front end of the car at $x = x_1$.
Because $x'_1 = L$, from \eqref{Lorentz-2} we get $$L = (x_1 - v\cdot 0)/(1 -
    v^2/c^2)^{1/2}$$ or $$x_1 = (1 - v^2/c^2)^{1/2}L < L.$$ So the doorman was
correct that the car fitted into the garage when they slammed the door.

Now assume the driver sees the back wall of the garage at $x' = x'_2$ when the
door is slammed at $t' = 0$. We plug the values $t'_2 = 0, x_2 = L$ into
\eqref{Lorentz-1} and \eqref{Lorentz-2} to get
\begin{align*}
    0    & = (t_2 - vL/c^2)/(1-v^2/c^2)^{1/2}, \\
    x'_2 & = (L - vt_2)/(1 - v^2/c^2)^{1/2},
\end{align*}
from which we solve
\begin{align*}
    t_2  & = vL/c^2,                   \\
    x'_2 & = (1 - v^2/c^2)^{1/2}L < L.
\end{align*}
So the driver is also correct.

If we want to analyze the case that the car decelerates and does not crash into
the back wall, we would have to introduce non-inertial coordinates.

\chapter{Manifolds and Tensor Fields}

\setcounter{section}{2}
\section{Tensors; the Metric Tensor}

\Page{17}

\begin{quotebar}
    Had we chosen a different chart, $\mathtt{\psi}'$, we would have obtained a
    different coordinate basis $\{X'_\nu\}$. We can, of course, express $X_\mu$ in
    terms of the new basis $\{X'_\nu\}$. Using the chain rule of advanced calculus,
    we have
    \begin{equation*}
        X_\mu = \sum_{\nu=1}^n \vat{\pdv{x'^\nu}{x^\mu}}{\psi(p)} X'_\nu,
    \end{equation*}
    where $x'^\nu$ denotes the $\nu$th component of the map $\psi'\circ\psi^{-1}$.
\end{quotebar}

\begin{proof}
    Let $f \in \mathscr{F}$, $X_\mu$ acting on $f$ gives rise to
    \begin{align*}
        X_\mu(f) & = \vat{\pdvf{x^\mu}{f\circ\psi^{-1}}}{\psi(p)}                          \\
                 & = \vat{\pdvf{x^\mu}{f\circ\psi'^{-1}\circ\psi'\circ\psi^{-1}}}{\psi(p)} \\
                 & = \sum_{\nu=1}^n \vat{\pdvf{x'^\nu}{f\circ\psi'^{-1}}}{\psi'(p)}
        \vat{\pdv{\prn{\psi'\circ\psi^{-1}}^\nu}{x^\mu}}{\psi(p)}                          \\
                 & = \sum_{\nu=1}^n \vat{\pdv{x'^\nu}{x^\mu}}{\psi(p)} \qedhere
        X'_\nu(f).
    \end{align*}
\end{proof}

\Page{20}

\begin{quotebar}
    For example, a tensor $T$ of type $(1,1)$ is a multilinear map from $V^* \times
        V \to R$. Hence, if we fix $v \in V$, $T(\cdot,v)$ is an element of $V^{**}$,
    which we identify with an element of $V$. Thus, given a vector in $V$, $T$
    produces another vector in $V$ in a linear fashion. In other words, we can view
    a tensor of type $(1,1)$ as a linear map from $V$ into $V$, and vice versa.
\end{quotebar}

The above quote associates a tensor to a linear map. We want to define the
reverse association and show it is the inverse of the forward association.
Actually, given any linear map $L\colon V \to V$, the map $(v^*, v) \mapsto
    v^*(L(v))$ defines a tensor of type $(1,1)$. Now assume $L$ is the linear map
associated with tensor $T$. For fixed $v \in V$, by the quote $T(\cdot,v)$ as
an element of $V^{**}$ is identified with $L(v)$, meaning for any $v^* \in
    V^*$, $T(v^*,v) = v^*(L(v))$. This shows the reverse association is indeed the
inverse of the forward one.

\setcounter{section}{3}
\section{The Abstract Index Notation}

\Page{25}

\begin{quotebar}
    Since a metric $g$ is a tensor of type $(0,2)$, it is denoted $g_{ab}$. If we
    apply the metric to a vector, $v^a$, we get the dual vector $g_{ab}v^b$.
\end{quotebar}

To make sense of $g_{ab}v^b$, we view $v$ as an element of $V_p^{**}$, thus a
tensor of type $(1,0)$ and $g_{ab}v^b$ as a contraction of the outer product of
$g$ and $v$. We want to show that the dual vector obtained by applying $g$ to
$v$, i.e., $u \mapsto g(u,v)$, is just $g_{ab}v^b$.
\begin{proof}
    Let $\set{w_\sigma}$ be a basis of $V_p$ and $\set{w^{\sigma^*}}$ its dual
    basis. For any $u \in V_p$, $g_{ab}v^b$ evaluated at $u$ by definition is
    \begin{equation}
        g_{ab}v^b(u) = \sum_{\sigma=1}^n g(u, w_\sigma)v\prn{w^{\sigma^*}} =
        \sum_{\sigma=1}^n g(u, w_\sigma)w^{\sigma^*}(v) = g\prn{u, \sum_{\sigma=1}^n
            w^{\sigma^*}(v) w_\sigma} = g(u,v). \label{eq:contraction-technique} \qedhere
    \end{equation}
\end{proof}

\begin{quotebar}
    The inverse of $g_{ab}$ is a tensor of type $(2,0)$ and could be denoted as
    $\prn{g^{-1}}^{ab}$. It is convenient, however, to drop the inverse sign and
    denote it simply as $g^{ab}$... by definition, $g^{ab}g_{bc} = {\delta^a}_c$,
    where ${\delta^a}_c$ (viewed as a map from $V_p$ into $V_p$) is the identity
    map.
\end{quotebar}

Let $L\colon V\to V^*$ denote the linear map induced by $g$, i.e., $g(u, v) =
    L(v)(u)$ for any $u, v \in V_p$. By definition, for any $u^*, v^* \in V_p^*$,
$g^{-1}(u^*, v^*) = L^{-1}(v^*)(u^*) = u^*\prn{L^{-1}(v^*)}$. Let
$\set{w_\sigma}$ be a basis of $V_p$ and $\set{w^{\sigma^*}}$ its dual basis.
Then for any $u^* \in V_p^*$, $v \in V_p$, we have
\begin{align}
    g^{ab}g_{bc}(u^*, v)
     & = \sum_{\sigma=1}^n g^{-1}\prn{u^*, w^{\sigma^*}} g\prn{w_\sigma, v} \notag     \\
     & = \sum_{\sigma=1}^n u^*\prn{L^{-1}\prn{w^{\sigma^*}}} L(v)\prn{w_\sigma} \notag \\
     & = \sum_{\sigma=1}^n u^*\prn{L(v)\prn{w_\sigma}L^{-1}\prn{w^{\sigma^*}}} \notag  \\
     & = \sum_{\sigma=1}^n u^*\prn{L^{-1}\prn{L(v)\prn{w_\sigma}w^{\sigma^*}}} \notag  \\
     & = u^*\prn{L^{-1}\prn{\sum_{\sigma=1}^n L(v)\prn{w_\sigma}w^{\sigma^*}}}.
    \label{eq:prove-metric-inverse}
\end{align}
Using the same technique used in \eqref{eq:contraction-technique}, one has for
all $x \in V_p$,
\begin{equation*}
    \sum_{\sigma=1}^n L(v)\prn{w_\sigma}w^{\sigma^*}(x) = \sum_{\sigma=1}^n
    L(v)\prn{w^{\sigma^*}(x) w_\sigma} = L(v)\prn{\sum_{\sigma=1}^n w^{\sigma^*}(x)
        w_\sigma} = L(v)(x),
\end{equation*}
thus the equality between the following two dual vectors,
\begin{equation*}
    \sum_{\sigma=1}^n L(v)\prn{w_\sigma}w^{\sigma^*} = L(v).
\end{equation*}
Plugging this into \eqref{eq:prove-metric-inverse}, we get
\begin{equation*}
    g^{ab}g_{bc}(u^*, v) = u^*\prn{L^{-1}(L(v))} = u^*(v) =
    {\delta^a}_c\prn{u^*,v}.
\end{equation*}

\begin{quotebar}
    In general, raised or lowered indices on any tensor denote application of the
    metric or inverse metric to that slot. Thus, for example, if $T^{abc}{}_{de}$
    is a tensor of type $(3,2)$, then $T^a{}_b{}^{cde}$ denotes the tensor
    $g_{bf}g^{dh}g^{ej}T^{afc}{}_{hj}$. This notation is self-consistent since the
    tensor resulting from the successive raising and lowering of a given index is
    identical to the original tensor.
\end{quotebar}

[TODO]

\begin{quotebar}
    Furthurmore, the notation is also self-consistent when applied to the metric
    itself, since $g^{ab} = g^{ac}g^{bd}g_{cd}$, i.e., $g_{ab}$ is the tensor
    $g_{ab}$ when it indices raised.
\end{quotebar}

[TODO]

\begin{quotebar}
    Similar notational rules apply to any pair of covariant or contravariant
    indices of tensors of higher type.
\end{quotebar}

Actually it can be induced that for a tensor $T_{a_1\cdots a_l}$ of type $(0,
    l)$, the new tensor in index notation $T_{a_{\pi_1}\cdots a_{\pi_l}}$ acts on
the vectors $\prn{{v^1}^a, \dots, {v^l}^a}$ as
\begin{equation*}
    T_{a_{\pi_1}\cdots a_{\pi_l}} \prn{{v^1}^a, \dots, {v^l}^a} = T_{a_1\cdots a_l}
    \prn{{v^{\pi^{-1}(1)}}^a, \dots, {v^{\pi^{-1}(l)}}^a}.
\end{equation*}
We show an example of why this is true. Take, the tensor $T_{abc}$ for example,
and consider the index notation $T_{bca}$. $T_{bca}$ can be obtained from
$T_{abc}$ by first interchanging indices $a$ and $b$ to get $T_{bac}$, then
indices $a$ and $c$ to get $T_{bca}$. Thus $T_{bca}$ acting on vectors
$\prn{u^a, v^a, w^a}$ is
\begin{equation*}
    T_{bca} \prn{u^a, v^a, w^a} = T_{bac} \prn{u^a, w^a, v^a} = T_{abc} \prn{w^a,
        u^a, v^a}.
\end{equation*}
We see we have applied on $\prn{u^a, v^a, w^a}$ the permutation that takes
$bca$ into $abc$, which is the inverse of the permutation that takes $abc$ into
$bca$.

\appendix

\titleformat{\chapter}[display] {\fontspec{Times New Roman}} {APPENDIX
    \Alph{chapter}} {1pt} {\hrule\vspace{6pt}\Large\fontspec{Times New
        Roman}\MakeUppercase}

\setcounter{chapter}{1}

\chapter{Differential Forms, Integration, and Frobenius's Theorem}

\section{Differential Forms}

\Page{428}

\begin{quotebar}
    We denote the vector space of $p$-forms at a point $x$ by $\Lambda_x^p$ and the
    collection of $p$-form fields by $\Lambda^p$. Note that $\Lambda_x^p = \set{0}$
    if $p > n$ and $\dim \Lambda_x^p = n!/p!(n-p)!$ for $0 \leq p \leq n$.
\end{quotebar}

By multilinearity of tensors and antisymmetry of differential forms, the value
of a $p$-form $\omega$ acting on $p$ vectors can be written as a linear
combination of $\omega\prn{v_{\mu_1},\dots,v_{\mu_p}}$, $0 \leq \mu_1 < \cdots
    < \mu_p \leq n$, where $\set{v_1,\dots,v_p}$ is a basis of $V_x$. When $p > n$,
such $\mu_1, \dots, \mu_p$ don't exist and $w = 0$. When $0 \leq p \leq n$,
there are $n!/p!(n-p)!$ unique combinations of $\mu_1, \dots, \mu_p$, and it's
not hard to go from here to show that $\setbuilder{{v^{\mu_1}}^* \wedge \cdots
        \wedge {v^{\mu_p}}^*}{0 \leq \mu_1 < \cdots < \mu_p \leq n}$ form a basis of
$\Lambda_x^p$.

\begin{quotebar}
    However, we can totally antisymmetrize this tensor, thus producing a map
    $\wedge\colon \Lambda_x^p\times\Lambda_x^q \to \Lambda_x^{p+q}$ via
    \begin{equation*}
        \prn{\omega\wedge\mu}_{a_1\cdots a_pb_1\cdots b_q} = \frac{(p+q)!}{p!q!}
        \omega_{\left[a_1\cdots a_p\right.}\mu_{\left.b_1\cdots b_q\right]}.
    \end{equation*}
\end{quotebar}

Side note: the notation of the totally antisymmetric part looks weird; why
don't we denote it by something like $\prn{\omega\mu}_{\left[a_1\cdots a_p
        b_1\cdots b_q\right]}$?

Theory: the constant $(p+q)!/p!q!$ is to make sure the exterior product
$\wedge$ is associative. [TODO]

\end{document}
