\documentclass{note}

\usepackage{mathrsfs}
\usepackage{geometry}
\usepackage{xcolor}
\usepackage{fontspec}
\usepackage{titlesec}
\usepackage{fmtcount}


\titleformat{\chapter}[display]
    {\fontspec{Times New Roman}}
    {\NUMBERstring{chapter}}
    {1pt}
    {\hrule\vspace{6pt}\Large\fontspec{Times New Roman}\MakeUppercase}
\renewcommand{\thesection}{\thechapter.\arabic{section}}
\titleformat{\section}[block]
    {\Large\bfseries}
    {\thesection}
    {1em}
    {\fontspec{Times New Roman}\selectfont}


\numberwithin{equation}{chapter}
\makeatletter
\def\tagform@#1{\maketag@@@{[\ignorespaces#1\unskip\@@italiccorr]}}
\makeatother

\newcounter{exercise}[chapter]
\newcommand\Ex{
    \if@noskipsec \leavevmode \fi
    \par
    \refstepcounter{exercise}
    \noindent{\bfseries\arabic{exercise}.}
}

\renewcommand{\theenumi}{\alph{enumi}}

\newcommand{\vat}[2]{\left.#1\right\rvert_{#2}}
\newcommand{\dv}[2]{\frac{d #1}{d #2}}
\newcommand{\pdv}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\pdvf}[2]{\frac{\partial}{\partial #1}\prn{#2}}
\newcommand{\dif}{\operatorname{d}\!}


\begin{document}

\chapter{Introduction}

\section*{Problem}

\Page{9}

\Ex \textit{Car and garage paradox.} We set up global inertial coordinates $t,
    x, y, z$ for the doorman and $t', x', y', z'$ for the driver. Assume that the
car travels in the $x$ and $x'$ directions in the two systems, that the garage
door is located at $x = y = z = 0$ in the doorman's coordinates and the back
end of the car is located at $x' = y' = z' = 0$ in the driver's coordinates,
and that the back end of the car enters the garage at time $t = t' = 0$. Also
assume that the car and the garage have the same length L.

We note down the Lorentz transform:
\begin{align}
    t' & = (t - vx/c^2)/(1-v^2/c^2)^{1/2}, \label{Lorentz-1} \\
    x' & = (x - vt)/(1 - v^2/c^2)^{1/2}. \label{Lorentz-2}
\end{align}

At $t = 0$, we assume the doorman sees the front end of the car at $x = x_1$.
Because $x'_1 = L$, from \eqref{Lorentz-2} we get $$L = (x_1 - v\cdot 0)/(1 -
    v^2/c^2)^{1/2}$$ or $$x_1 = (1 - v^2/c^2)^{1/2}L < L.$$ So the doorman was
correct that the car fitted into the garage when they slammed the door.

Now assume the driver sees the back wall of the garage at $x' = x'_2$ when the
door is slammed at $t' = 0$. We plug the values $t'_2 = 0, x_2 = L$ into
\eqref{Lorentz-1} and \eqref{Lorentz-2} to get
\begin{align*}
    0    & = (t_2 - vL/c^2)/(1-v^2/c^2)^{1/2}, \\
    x'_2 & = (L - vt_2)/(1 - v^2/c^2)^{1/2},
\end{align*}
from which we solve
\begin{align*}
    t_2  & = vL/c^2,                   \\
    x'_2 & = (1 - v^2/c^2)^{1/2}L < L.
\end{align*}
So the driver is also correct.

If we want to analyze the case that the car decelerates and does not crash into
the back wall, we would have to introduce non-inertial coordinates.

\chapter{Manifolds and Tensor Fields}

\setcounter{section}{1}

\section{Vectors}

\Page{17}

\begin{quotebar}
    Had we chosen a different chart, $\mathtt{\psi}'$, we would have obtained a
    different coordinate basis $\{X'_\nu\}$. We can, of course, express $X_\mu$ in
    terms of the new basis $\{X'_\nu\}$. Using the chain rule of advanced calculus,
    we have
    \begin{equation*}
        X_\mu = \sum_{\nu=1}^n \vat{\pdv{x'^\nu}{x^\mu}}{\psi(p)} X'_\nu,
    \end{equation*}
    where $x'^\nu$ denotes the $\nu$th component of the map $\psi'\circ\psi^{-1}$.
\end{quotebar}

\begin{proof}
    Let $f \in \mathscr{F}$, $X_\mu$ acting on $f$ gives rise to
    \begin{align*}
        X_\mu(f) & = \vat{\pdvf{x^\mu}{f\circ\psi^{-1}}}{\psi(p)}                          \\
                 & = \vat{\pdvf{x^\mu}{f\circ\psi'^{-1}\circ\psi'\circ\psi^{-1}}}{\psi(p)} \\
                 & = \sum_{\nu=1}^n \vat{\pdvf{x'^\nu}{f\circ\psi'^{-1}}}{\psi'(p)}
        \vat{\pdv{\prn{\psi'\circ\psi^{-1}}^\nu}{x^\mu}}{\psi(p)}                          \\
                 & = \sum_{\nu=1}^n \vat{\pdv{x'^\nu}{x^\mu}}{\psi(p)} \qedhere
        X'_\nu(f).
    \end{align*}
\end{proof}

\section{Tensors; the Metric Tensor}

\Page{20}

\begin{quotebar}
    For example, a tensor $T$ of type $(1,1)$ is a multilinear map from $V^* \times
        V \to R$. Hence, if we fix $v \in V$, $T(\cdot,v)$ is an element of $V^{**}$,
    which we identify with an element of $V$. Thus, given a vector in $V$, $T$
    produces another vector in $V$ in a linear fashion. In other words, we can view
    a tensor of type $(1,1)$ as a linear map from $V$ into $V$, and vice versa.
\end{quotebar}

The above quote associates a tensor to a linear map. We want to define the
reverse association and show it is the inverse of the forward association.
Actually, given any linear map $L\colon V \to V$, the map $(w^*, v) \mapsto
    w^*(L(v))$ defines a tensor of type $(1,1)$. Now assume $L$ is the linear map
associated with tensor $T$. For fixed $v \in V$, by the quote $T(\cdot,v)$ as
an element of $V^{**}$ is identified with $L(v)$, meaning for any $w^* \in
    V^*$, $T(w^*,v) = w^*(L(v))$. This shows the reverse association is indeed the
inverse of the forward one.

\setcounter{section}{3}
\section{The Abstract Index Notation}

\Page{25}

\begin{quotebar}
    Since a metric $g$ is a tensor of type $(0,2)$, it is denoted $g_{ab}$. If we
    apply the metric to a vector, $v^a$, we get the dual vector $g_{ab}v^b$.
\end{quotebar}

To make sense of $g_{ab}v^b$, we view $v^a$ as an element of $V_p^{**}$, thus a
tensor of type $(1,0)$ and $g_{ab}v^b$ as a contraction of the outer product
between $g_{ab}$ and $v^b$. We want to show that $g_{ab}v^b$ is just the dual
vector obtained by applying $g_{ab}$ to $v^a$, i.e., $g_{ab}(\cdot,v^a)$.
\begin{proof}
    Let $\set{w_\sigma}$ be a basis of $V_p$ and $\set{{w^\sigma}^*}$ its dual
    basis. For any $u^a\in V_p$, $g_{ab}v^b$ evaluated at $u^a$ by the definition
    of contraction is
    \begin{align}
        g_{ab}v^b(u^a)
         & = \sum_{\sigma=1}^n g_{ab}(u^a, w_\sigma)v^a\prn{{w^\sigma}^*} =
        \sum_{\sigma=1}^n g_{ab}(u^a, w_\sigma){w^\sigma}^*(v^a)
        = g_{ab}\prn{u^a, \sum_{\sigma=1}^n {w^\sigma}^*(v^a) w_\sigma}
        \label{eq:contraction-technique}                                    \\
         & = g_{ab}(u^a,v^a). \notag \qedhere
    \end{align}
\end{proof}

\begin{quotebar}
    The inverse of $g_{ab}$ is a tensor of type $(2,0)$ and could be denoted as
    $\prn{g^{-1}}^{ab}$. It is convenient, however, to drop the inverse sign and
    denote it simply as $g^{ab}$... by definition, $g^{ab}g_{bc} = {\delta^a}_c$,
    where ${\delta^a}_c$ (viewed as a map from $V_p$ into $V_p$) is the identity
    map.
\end{quotebar}

Let $L\colon V\to V^*$ denote the linear map induced by $g$, i.e., $g(u, v) =
    L(v)(u)$ for any $u, v \in V_p$. By definition, for any $u^*, v^* \in V_p^*$,
$g^{-1}(u^*, v^*) = L^{-1}(v^*)(u^*) = u^*\prn{L^{-1}(v^*)}$. Let
$\set{w_\sigma}$ be a basis of $V_p$ and $\set{{w^\sigma}^*}$ its dual basis.
Then for any $u^* \in V_p^*$, $v \in V_p$, we have
\begin{align}
    g^{ab}g_{bc}(u^*, v)
     & = \sum_{\sigma=1}^n g^{-1}\prn{u^*, {w^\sigma}^*} g\prn{w_\sigma, v} \notag     \\
     & = \sum_{\sigma=1}^n u^*\prn{L^{-1}\prn{{w^\sigma}^*}} L(v)\prn{w_\sigma} \notag \\
     & = \sum_{\sigma=1}^n u^*\prn{L(v)\prn{w_\sigma}L^{-1}\prn{{w^\sigma}^*}} \notag  \\
     & = \sum_{\sigma=1}^n u^*\prn{L^{-1}\prn{L(v)\prn{w_\sigma}{w^\sigma}^*}} \notag  \\
     & = u^*\prn{L^{-1}\prn{\sum_{\sigma=1}^n L(v)\prn{w_\sigma}{w^\sigma}^*}}.
    \label{eq:prove-metric-inverse}
\end{align}
Using the same technique used in \eqref{eq:contraction-technique}, one has for
all $x \in V_p$,
\begin{equation*}
    \sum_{\sigma=1}^n L(v)\prn{w_\sigma}{w^\sigma}^*(x) = \sum_{\sigma=1}^n
    L(v)\prn{{w^\sigma}^*(x) w_\sigma} = L(v)\prn{\sum_{\sigma=1}^n {w^\sigma}^*(x)
    w_\sigma} = L(v)(x),
\end{equation*}
thus the equality between the following two dual vectors,
\begin{equation*}
    \sum_{\sigma=1}^n L(v)\prn{w_\sigma}{w^\sigma}^* = L(v).
\end{equation*}
Plugging this into \eqref{eq:prove-metric-inverse}, we get
\begin{equation*}
    g^{ab}g_{bc}(u^*, v) = u^*\prn{L^{-1}(L(v))} = u^*(v) =
    {\delta^a}_c\prn{u^*,v}.
\end{equation*}

{\bfseries In fact, we might be able to prove the following general result.}
\begin{lemma*}
    Let ${T^{a_1\cdots a_k}}_{b_1\cdots b_l}$ be a tensor of type $(k,l)$ and $v^a$
    be a vector. Then ${T^{a_1\cdots a_k}}_{b_1\cdots b_l}v^{b_i}$ behaves like
    plugging $v^a$ into the $b_i$-th slot of ${T^{a_1\cdots a_k}}_{b_1\cdots b_l}$.
    The same holds true for a dual vector $\omega_a$ and ${T^{a_1\cdots
                        a_k}}_{b_1\cdots b_l}\omega_{a_j}$.
\end{lemma*}

\begin{proof}
    [TODO]
\end{proof}

\begin{quotebar}
    In general, raised or lowered indices on any tensor denote application of the
    metric or inverse metric to that slot. Thus, for example, if $T^{abc}{}_{de}$
    is a tensor of type $(3,2)$, then $T^a{}_b{}^{cde}$ denotes the tensor
    $g_{bf}g^{dh}g^{ej}T^{afc}{}_{hj}$. This notation is self-consistent since the
    tensor resulting from the successive raising and lowering of a given index is
    identical to the original tensor.
\end{quotebar}

[TODO]

\begin{quotebar}
    Furthurmore, the notation is also self-consistent when applied to the metric
    itself, since $g^{ab} = g^{ac}g^{bd}g_{cd}$, i.e., $g_{ab}$ is the tensor
    $g_{ab}$ when it indices raised.
\end{quotebar}

[TODO]

\begin{quotebar}
    Similar notational rules apply to any pair of covariant or contravariant
    indices of tensors of higher type.
\end{quotebar}

Actually it can be induced that for a tensor $T_{a_1\cdots a_l}$ of type $(0,
    l)$, the new tensor in index notation $T_{a_{\pi_1}\cdots a_{\pi_l}}$ acts on
the vectors $\prn{{v^1}^a, \dots, {v^l}^a}$ as
\begin{equation*}
    T_{a_{\pi_1}\cdots a_{\pi_l}} \prn{{v^1}^a, \dots, {v^l}^a} = T_{a_1\cdots a_l}
    \prn{{v^{\pi^{-1}(1)}}^a, \dots, {v^{\pi^{-1}(l)}}^a}.
\end{equation*}
We show an example of why this is true. Take, the tensor $T_{abc}$ for example,
and consider the index notation $T_{bca}$. $T_{bca}$ can be obtained from
$T_{abc}$ by first interchanging indices $a$ and $b$ to get $T_{bac}$, then
indices $a$ and $c$ to get $T_{bca}$. Thus $T_{bca}$ acting on vectors
$\prn{u^a, v^a, w^a}$ is
\begin{equation*}
    T_{bca} \prn{u^a, v^a, w^a} = T_{bac} \prn{u^a, w^a, v^a} = T_{abc} \prn{w^a,
        u^a, v^a}.
\end{equation*}
We see we have applied on $\prn{u^a, v^a, w^a}$ the permutation that takes
$bca$ into $abc$, which is the inverse of the permutation that takes $abc$ into
$bca$.

\section*{Problems}

\Page{27}

\setcounter{exercise}{2}

\Ex a) Linearity should be straightforward. To prove the Leibnitz property, let
$f, g\in \mathscr{F}$. We have
\begin{align*}
    [v,w](fg)
     & = v(w(fg)) - w(v(fg))                             \\
     & = v(fw(g) + gw(f)) - w(fv(g) + gv(f))             \\
     & = v(fw(g)) + v(gw(f)) - w(fv(g)) - w(gv(f))       \\
     & = fv(w(g)) + v(f)w(g) + gv(w(f)) + w(f)v(g)       \\
     & \quad - fw(v(g)) - w(f)v(g) - gw(v(f)) - v(f)w(g) \\
     & = fv(w(g)) + gv(w(f)) - fw(v(g)) - gw(v(f))       \\
     & = f(v(w(g)) - w(v(g))) + g(v(w(f)) - w(v(f)))     \\
     & = f[v,w](g) + g[v,w](f).
\end{align*}

b) Let $f \in \mathscr{F}$, then
\begin{align*}
    [[X,Y],Z](f)
     & = [X,Y](Z(f)) - Z([X,Y](f))                          \\
     & = X(Y(Z(f))) - Y(X(Z(f))) - Z(X(Y(f)) - Y(X(f)))     \\
     & = X(Y(Z(f))) - Z(X(Y(f))) + Z(Y(X(f))) - Y(X(Z(f))).
\end{align*}
Similarly,
\begin{align*}
    [[Y,Z],X](f) & = Y(Z(X(f))) - X(Y(Z(f))) + X(Z(Y(f))) - Z(Y(X(f))), \\
    [[Z,X],Y](f) & = Z(X(Y(f))) - Y(Z(X(f))) + Y(X(Z(f))) - X(Z(Y(f))).
\end{align*}
And it is easily seen that $[[X,Y],Z](f) + [[Y,Z],X](f) + [[Z,X],Y](f) = 0$.

c)
\begin{align*}
    [[Y_\alpha,Y_\beta], Y_\gamma]
     & = \left[ \sum_\mu {C^\mu}_{\alpha\beta} Y_\mu, Y_\gamma \right]
    = \sum_\mu {C^\mu}_{\alpha\beta} [Y_\mu, Y_\gamma]
    = \sum_\mu {C^\mu}_{\alpha\beta} \sum_\nu {C^\nu}_{\mu\gamma} Y_\nu          \\
     & = \sum_\nu \prn{\sum_\mu {C^\nu}_{\mu\gamma} {C^\mu}_{\alpha\beta}} Y_\nu
\end{align*}
Thus by applying Jacobi identity to $Y_\alpha, Y_\beta, Y_\gamma$ we get
\begin{equation*}
    \sum_\mu \prn{ {C^\nu}_{\mu\gamma} {C^\mu}_{\alpha\beta} + {C^\nu}_{\mu\alpha}
    {C^\mu}_{\beta\gamma} + {C^\nu}_{\mu\beta} {C^\mu}_{\gamma\alpha} } = 0, \
    \forall \nu.
\end{equation*}
See \cite{lee2009manifolds}, page 206, equation (5.1) ii) that this is correct.

\Ex a) By equation (2.2.7) on page 16, the coefficients $[v,w]^\mu$ are the
values of $[v,w]$ applied to the function $x^\mu\circ\psi$,
\begin{align*}
    [v,w]^\mu
     & = [v,w]\prn{x^\mu\circ\psi}
    = v\prn{w(x^\mu\circ\psi)} - w\prn{v(x^\mu\circ\psi)} \\
     & = v\prn{w^\mu} - w\prn{v^\mu}
    = \sum_{\nu=1}^n \prn{ v^\nu \pdv{w^\mu}{x^\nu} -
        w^\nu\pdv{v^\mu}{x^v} }.
\end{align*}

b) Applying ${Y^\gamma}^*$ to both sides of
\begin{equation*}
    [Y_\alpha, Y_\beta] = \sum_\mu {C^\mu}_{\alpha\beta} Y_\mu,
\end{equation*}
we get
\begin{equation*}
    {Y^\gamma}^*([Y_\alpha, Y_\beta]) =
    {Y^\gamma}^*\prn{\sum_\mu {C^\mu}_{\alpha\beta} Y_\mu} = {C^\gamma}_{\alpha\beta}.
\end{equation*}
Multiplying both sides by $({Y^\alpha}^*)_\mu({Y^\beta}^*)_\nu$ we get
\begin{equation}
    {Y^\gamma}^*([Y_\alpha, Y_\beta]) ({Y^\alpha}^*)_\mu({Y^\beta}^*)_\nu =
    {C^\gamma}_{\alpha\beta} ({Y^\alpha}^*)_\mu({Y^\beta}^*)_\nu.
    \label{chap2problem4b}
\end{equation}
One would be tempted to sum both sides over $\alpha,\beta$ and compute the
left-hand side as
\begin{align*}
    \sum_{\alpha,\beta} {Y^\gamma}^*([Y_\alpha, Y_\beta])
    ({Y^\alpha}^*)_\mu({Y^\beta}^*)_\nu
     & = \sum_{\alpha,\beta} {Y^\gamma}^*\prn{\left[({Y^\alpha}^*)_\mu
    Y_\alpha, ({Y^\beta}^*)_\nu Y_\beta\right]}                             \\
     & = {Y^\gamma}^*\prn{\sum_{\alpha,\beta} \left[({Y^\alpha}^*)_\mu
    Y_\alpha, ({Y^\beta}^*)_\nu Y_\beta\right]}                             \\
     & = {Y^\gamma}^*\prn{\sum_\alpha\sum_\beta \left[({Y^\alpha}^*)_\mu
    Y_\alpha, ({Y^\beta}^*)_\nu Y_\beta\right]}                             \\
     & = {Y^\gamma}^*\prn{\sum_\alpha \left[({Y^\alpha}^*)_\mu
    Y_\alpha, \sum_\beta ({Y^\beta}^*)_\nu Y_\beta\right]}                  \\
     & = {Y^\gamma}^*\prn{\left[\sum_\alpha ({Y^\alpha}^*)_\mu
    Y_\alpha, \sum_\beta ({Y^\beta}^*)_\nu Y_\beta\right]}                  \\
     & = {Y^\gamma}^*\prn{\left[\sum_\alpha {Y^\alpha}^*\prn{\pdv{}{x^\mu}}
    Y_\alpha, \sum_\beta {Y^\beta}^*\prn{\pdv{}{x^\nu}} Y_\beta\right]}     \\
     & = {Y^\gamma}^*\prn{\left[\pdv{}{x^\mu}, \pdv{}{x^\nu}\right]}
    = {Y^\gamma}^*(0) = 0.                                                  \\
\end{align*}
However the first equality is incorrect as $({Y^\alpha}^*)_\mu$ and
$({Y^\beta}^*)_\nu$ are not constants and do not commute with the commutator of
vector fields. But we can fix this by observing that
\begin{align*}
    \left[({Y^\alpha}^*)_\mu Y_\alpha, ({Y^\beta}^*)_\nu Y_\beta\right](f)
     & = ({Y^\alpha}^*)_\mu Y_\alpha\prn{({Y^\beta}^*)_\nu Y_\beta(f)} -
    ({Y^\beta}^*)_\nu Y_\beta\Big(({Y^\alpha}^*)_\mu Y_\alpha(f)\Big)    \\
    \text{(by Leibnitz rule) }
     & =
    ({Y^\alpha}^*)_\mu ({Y^\beta}^*)_\nu Y_\alpha\Big(Y_\beta(f)\Big)
    + ({Y^\alpha}^*)_\mu Y_\beta(f) Y_\alpha\prn{({Y^\beta}^*)_\nu}      \\
     & \quad
    - ({Y^\beta}^*)_\nu ({Y^\alpha}^*)_\mu Y_\beta\Big(Y_\alpha(f)\Big)
    - ({Y^\beta}^*)_\nu Y_\alpha(f) Y_\beta\Big(({Y^\alpha}^*)_\mu\Big)  \\
     & =
    ({Y^\alpha}^*)_\mu ({Y^\beta}^*)_\nu [Y_\alpha, Y_\beta](f)
    + ({Y^\alpha}^*)_\mu Y_\alpha\prn{({Y^\beta}^*)_\nu} Y_\beta(f)      \\
     & \quad
    - ({Y^\beta}^*)_\nu Y_\beta\Big(({Y^\alpha}^*)_\mu\Big) Y_\alpha(f). \\
\end{align*}
Thus
\begin{align*}
    \left[({Y^\alpha}^*)_\mu Y_\alpha, ({Y^\beta}^*)_\nu Y_\beta\right]
     & = ({Y^\alpha}^*)_\mu ({Y^\beta}^*)_\nu [Y_\alpha, Y_\beta]
    + ({Y^\alpha}^*)_\mu Y_\alpha\prn{({Y^\beta}^*)_\nu} Y_\beta      \\
     & \quad
    - ({Y^\beta}^*)_\nu Y_\beta\Big(({Y^\alpha}^*)_\mu\Big) Y_\alpha. \\
\end{align*}
Now applying ${Y^\gamma}^*$ to both sides and summing over $\alpha, \beta$, and
utilizing the linearity of vectors, we get
\begin{equation*}
    0 = \sum_{\alpha,\beta}
    {Y^\gamma}^*([Y_\alpha, Y_\beta]) ({Y^\alpha}^*)_\mu({Y^\beta}^*)_\nu
    + \sum_\alpha ({Y^\alpha}^*)_\mu Y_\alpha\Big(({Y^\gamma}^*)_\nu\Big)
    - \sum_\beta ({Y^\beta}^*)_\nu Y_\beta\Big(({Y^\gamma}^*)_\mu\Big),
\end{equation*}
i.e.,
\begin{align*}
    \sum_{\alpha,\beta}
    {Y^\gamma}^*([Y_\alpha, Y_\beta]) ({Y^\alpha}^*)_\mu({Y^\beta}^*)_\nu
     & = \sum_\beta ({Y^\beta}^*)_\nu Y_\beta\Big(({Y^\gamma}^*)_\mu\Big)
    - \sum_\alpha ({Y^\alpha}^*)_\mu Y_\alpha\Big(({Y^\gamma}^*)_\nu\Big)   \\
     & = \sum_\beta {Y^\beta}^*\prn{\pdv{}{x^\nu}}Y_\beta
    \Big(({Y^\gamma}^*)_\mu\Big)
    - \sum_\alpha {Y^\alpha}^*\prn{\pdv{}{x^\mu}}Y_\alpha
    \Big(({Y^\gamma}^*)_\nu\Big)                                            \\
     & = \prn{\sum_\beta {Y^\beta}^*\prn{\pdv{}{x^\nu}}Y_\beta}
    \Big(({Y^\gamma}^*)_\mu\Big)                                            \\
     & \quad - \prn{\sum_\alpha {Y^\alpha}^*\prn{\pdv{}{x^\mu}}Y_\alpha}
    \Big(({Y^\gamma}^*)_\nu\Big)                                            \\
     & = \pdv{}{x^\nu}\Big(({Y^\gamma}^*)_\mu\Big)
    - \pdv{}{x^\mu}\Big(({Y^\gamma}^*)_\nu\Big)                             \\
     & = \pdv{({Y^\gamma}^*)_\mu}{x^\nu} - \pdv{({Y^\gamma}^*)_\nu}{x^\mu}.
\end{align*}
This together with \eqref{chap2problem4b} proves that
\begin{equation*}
    \pdv{({Y^\gamma}^*)_\mu}{x^\nu} - \pdv{({Y^\gamma}^*)_\nu}{x^\mu} =
    \sum_{\alpha,\beta} {C^\gamma}_{\alpha\beta}
    ({Y^\alpha}^*)_\mu({Y^\beta}^*)_\nu.
\end{equation*}

\Ex Observe that the equations
\begin{equation*}
    \pdv{f^\nu}{x^\mu} = ({Y^\nu}^*)_\mu, \quad \mu = 1, \cdots, n,
\end{equation*}
for functions $f^\nu, \nu = 1, \cdots, n$ have solutions in an open ball $B$ in
$\reals^n$ that contains $\psi(p)$ (assuming $\psi$ is the chart in discussion)
due to 4(b). By the inverse function theorem, there exists an open neighborhood
$U$ of $\psi(p)$ in $B$ in which the map $M\colon x \mapsto
    \prn{f^1(x),\dots,f^n(x)}$ is invertible. We prove that the coordinate system
in $\psi^{-1}(U)$ defined by $p \mapsto M\circ\psi(p)$ satisfies the
requirements of the problem. In fact, by the inverse function theorem, the
Jacobian matrix of $M^{-1}$ at $y=M(x)$ is invertible to the Jacobian matrix of
$M$ at $x$, i.e.,
\begin{equation}
    \label{chap2problem5} \prn{\pdv{x^\nu}{y^\mu}}_{\nu,\mu} =
    {\prn{({Y^\nu}^*)_\mu}_{\nu,\mu}}^{-1}.
\end{equation}
By the definition of a dual basis, ${Y^\nu}^*(Y_\mu) = \delta_{\mu\nu}$ where
$\delta_{\mu\nu}$ is the identity. On the other hand, ${Y^\nu}^*(Y_\mu) =
    \sum_{\lambda=1}^n({Y^\nu}^*)_\lambda(Y_\mu)^\lambda$. So we get
$\sum_{\lambda=1}^n({Y^\nu}^*)_\lambda(Y_\mu)^\lambda = \delta_{\mu\nu}$, i.e.,
the inverse of the matrix $\prn{({Y^\nu}^*)_\mu}_{\nu,\mu}$ is just
$\prn{\prn{Y_\mu}^\nu}_{\nu,\mu}$. Combining this with \eqref{chap2problem5} we
get
\begin{equation*}
    \pdv{x^\nu}{y^\mu} = \prn{Y_\mu}^\nu, \quad \text{for all } \mu,\nu.
\end{equation*}
Plugging this into the vector transformation law (equation (2.2.9) on page 17)
we get
\begin{equation*}
    \pdv{}{y^\mu} = \sum_{\nu=1}^n \pdv{x^\nu}{y^\mu} X_\nu = \sum_{\nu=1}^n
    \prn{Y_\mu}^\nu X_\nu = Y_\mu,
\end{equation*}
as desired.

\Ex c) Let $\set{w_\sigma}$ be another basis of $V$ and $\set{{w^\sigma}^*}$
its dual basis. Then $T$ contracted by $\set{w_\sigma}$ is
\begin{align*}
    CT
     & = \sum_\sigma T\prn{\dots, {w^\sigma}^*, \dots\colon \dots,
    w_\sigma, \dots}                                                       \\
     & = \sum_\sigma T\prn{\dots, \sum_{\alpha} {w^\sigma}^*(v_\alpha)
        {v^\alpha}^*, \dots\colon \dots, \sum_{\beta} {v^\beta}^*(w_\sigma)
    v_\beta, \dots}                                                        \\
     & = \sum_{\alpha} \sum_{\beta} \sum_\sigma {w^\sigma}^*(v_\alpha)
    {v^\beta}^*(w_\sigma) T\prn{\dots, {v^\alpha}^*, \dots\colon \dots,
    v_\beta, \dots}                                                        \\
     & = \sum_{\alpha} \sum_{\beta} \sum_\sigma {v^\beta}^*
    \prn{{w^\sigma}^*(v_\alpha)w_\sigma} T\prn{\dots, {v^\alpha}^*,
    \dots\colon \dots, v_\beta, \dots}                                     \\
     & = \sum_{\alpha} \sum_{\beta} {v^\beta}^*
    \prn{\sum_\sigma {w^\sigma}^*(v_\alpha)w_\sigma} T\prn{\dots, {v^\alpha}^*,
    \dots\colon \dots, v_\beta, \dots}                                     \\
     & = \sum_{\alpha} \sum_{\beta} {v^\beta}^*\prn{v_\alpha} T\prn{\dots,
    {v^\alpha}^*, \dots\colon \dots, v_\beta, \dots}                       \\
     & = \sum_{\alpha} \sum_{\beta} \delta_{\alpha\beta} T\prn{\dots,
    {v^\alpha}^*, \dots\colon \dots, v_\beta, \dots}                       \\
     & = \sum_{\alpha} T\prn{\dots, {v^\alpha}^*, \dots\colon \dots,
        v_\alpha, \dots}.
\end{align*}

\Ex a) It is evident that
\begin{align*}
    x & = r\sin\theta\cos\phi, \\
    y & = r\sin\theta\sin\phi, \\
    z & = r\cos\theta.
\end{align*}
According to page 22 equation (2.3.7), the dual basis $\dif x$, $\dif y$, $\dif
    z$ can be expressed as
\begin{align*}
    \dif x & = \sin\theta\cos\phi\dif r + r\cos\theta\cos\phi\dif\theta
    - r\sin\theta\sin\phi\dif\phi                                       \\
    \dif y & = \sin\theta\sin\phi\dif r + r\cos\theta\sin\phi\dif\theta
    + r\sin\theta\cos\phi\dif\phi                                       \\
    \dif z & = \cos\theta\dif r - r\sin\theta\dif\theta.
\end{align*}
Thus
\begin{align*}
    \dif s^2
     & = \dif x^2 + \dif y^2 + \dif z^2                                  \\
     & = (\sin\theta\cos\phi\dif r + r\cos\theta\cos\phi\dif\theta
    - r\sin\theta\sin\phi\dif\phi)^2                                     \\
     & \quad + (\sin\theta\sin\phi\dif r + r\cos\theta\sin\phi\dif\theta
    + r\sin\theta\cos\phi\dif\phi)^2                                     \\
     & \quad + (\cos\theta\dif r - r\sin\theta\dif\theta)^2              \\
     & = \big[\sin^2\theta\cos^2\phi\dif r^2
        + r^2\cos^2\theta\cos^2\phi\dif\theta^2
    + r^2\sin^2\theta\sin^2\phi\dif\phi^2                                \\
     & \qquad + r\sin\theta\cos\theta\cos^2\phi(\dif r\otimes\dif\theta
        + \dif\theta\otimes\dif r) - r\sin^2\theta\sin\phi\cos\phi
    (\dif r\otimes\dif\phi + \dif\phi\otimes\dif r)                      \\
     & \qquad - r^2\sin\theta\cos\theta\sin\phi\cos\phi(\dif\theta
    \otimes \dif\phi + \dif\phi\otimes\dif\theta) \big]                  \\
     & + \big[\sin^2\theta\sin^2\phi\dif r^2
        + r^2\cos^2\theta\sin^2\phi\dif\theta^2
    + r^2\sin^2\theta\cos^2\phi\dif\phi^2                                \\
     & \qquad + r\sin\theta\cos\theta\sin^2\phi(\dif r\otimes\dif\theta
        + \dif\theta\otimes\dif r) + r\sin^2\theta\sin\phi\cos\phi
    (\dif r\otimes\dif\phi + \dif\phi\otimes\dif r)                      \\
     & \qquad - r^2\sin\theta\cos\theta\sin\phi\cos\phi(\dif\theta
    \otimes \dif\phi + \dif\phi\otimes\dif\theta) \big]                  \\
     & + \big[ \cos^2\theta\dif r^2 + r^2\sin^2\theta\dif\theta^2
        - r\cos\theta\sin\theta(\dif r\otimes\dif\theta +
    \dif\theta\otimes\dif r) \big]                                       \\
     & = \dif r^2 + r^2\dif\theta^2 + r^2\sin^2\theta\dif\phi^2.
\end{align*}

\chapter{Curvature}

\setcounter{section}{1}

\section{Curvature}

\Page{39}

\begin{quotebar}
    3. For the derivative operator $\nabla_a$ naturally associated with the
    metric, $\nabla_ag_{bc}=0$, we have
    \begin{equation*}
        R_{abcd} = - R_{abdc}.
    \end{equation*}
\end{quotebar}
Here we give a different proof from the one given in the book. First we note by
definition that $R_{abcd} = g_{de}R_{abc}{}^e$. For an arbitrary vector field
$v^a$, we apply this tensor to $v^a$ and use the definition of the Riemann
curvature tensor (3.2.3) to get that
\begin{align}
    g_{de}R_{abc}{}^ev^d
     & = R_{abc}{}^eg_{de}v^d = \nabla_a\nabla_bg_{dc}v^d
    - \nabla_b\nabla_ag_{dc}v^d \nonumber                                \\
     & = g_{dc}\nabla_a\nabla_bv^d - g_{dc}\nabla_b\nabla_av^d
    \label{(3.2.15)-proof-label-1}                                       \\
     & = g_{dc}\prn{\nabla_a\nabla_bv^d - \nabla_b\nabla_av^d} \nonumber \\
     & = -g_{dc}R_{abe}{}^dv^e \label{(3.2.15)-proof-label-2}            \\
     & = -g_{ec}R_{abd}{}^ev^d \label{(3.2.15)-proof-label-3}            \\
     & = -R_{abdc}v^d \nonumber
\end{align}
where in \eqref{(3.2.15)-proof-label-1} we used the Leibnitz rule of derivative
operators and the fact that $\nabla_ag_{bc}=0$, in
\eqref{(3.2.15)-proof-label-2} we used the property (3.2.11), and in
\eqref{(3.2.15)-proof-label-3} we applied an index substitution $d
    \leftrightarrow e$. Thus $R_{abcd}v^d = - R_{abdc}v^d$ for all $v^d$, which
proves the property.

\appendix

\titleformat{\chapter}[display] {\fontspec{Times New Roman}} {APPENDIX
    \Alph{chapter}} {1pt} {\hrule\vspace{6pt}\Large\fontspec{Times New
        Roman}\MakeUppercase}

\setcounter{chapter}{1}

\chapter{Differential Forms, Integration, and Frobenius's Theorem}

\section{Differential Forms}

\Page{428}

\begin{quotebar}
    We denote the vector space of $p$-forms at a point $x$ by $\Lambda_x^p$ and the
    collection of $p$-form fields by $\Lambda^p$. Note that $\Lambda_x^p = \set{0}$
    if $p > n$ and $\dim \Lambda_x^p = n!/p!(n-p)!$ for $0 \leq p \leq n$.
\end{quotebar}

By multilinearity of tensors and antisymmetry of differential forms, the value
of a $p$-form $\omega$ acting on $p$ vectors can be written as a linear
combination of $\omega\prn{v_{\mu_1},\dots,v_{\mu_p}}$, $0 \leq \mu_1 < \cdots
    < \mu_p \leq n$, where $\set{v_1,\dots,v_p}$ is a basis of $V_x$. When $p > n$,
such $\mu_1, \dots, \mu_p$ don't exist and $w = 0$. When $0 \leq p \leq n$,
there are $n!/p!(n-p)!$ unique combinations of $\mu_1, \dots, \mu_p$, and it's
not hard to go from here to show that $\setbuilder{{v^{\mu_1}}^* \wedge \cdots
        \wedge {v^{\mu_p}}^*}{0 \leq \mu_1 < \cdots < \mu_p \leq n}$ form a basis of
$\Lambda_x^p$.

\begin{quotebar}
    However, we can totally antisymmetrize this tensor, thus producing a map
    $\wedge\colon \Lambda_x^p\times\Lambda_x^q \to \Lambda_x^{p+q}$ via
    \begin{equation*}
        \prn{\omega\wedge\mu}_{a_1\cdots a_pb_1\cdots b_q} = \frac{(p+q)!}{p!q!}
        \omega_{\left[a_1\cdots a_p\right.}\mu_{\left.b_1\cdots b_q\right]}.
    \end{equation*}
\end{quotebar}

Side note: the notation of the totally antisymmetric part looks weird; why
don't we denote it by something like $\prn{\omega\mu}_{\left[a_1\cdots a_p
        b_1\cdots b_q\right]}$?

Theory: the constant $(p+q)!/p!q!$ is to make sure the exterior product
$\wedge$ is associative. [TODO]

\bibliographystyle{plain}

\bibliography{Wald-General_Relativity}

\end{document}
